# -*- coding: utf-8 -*-
"""
Created on Fri Jul  9 12:52:41 2021

@author: Fmny
"""
"""programm which is a python version about Tinatic.r"""


#vérification de la version de scikit-learn
import sklearn
print(sklearn.__version__)


#### Libraries

import os

#affiche le répertoire courant:
os.getcwd()

path = "C:/Users/Utilisateur/Documents/Python Scripts/Titanic"

path2 = os.getcwd()

#path = path2

path3="C:/Users/Utilisateur/Documents/Python Scripts/Titanic/data/"

#lien utilisation de la fonction sklearn qui remplace rpart sous R
#lien: http://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/fr_Tanagra_Scikit_Learn_Decision_Tree.pdf



#importation du jeu de données
#utilisation de pandas
import pandas as pd
titanic1=pd.read_csv(path3 +'titanic.csv',sep=',' ,header=0,encoding='utf-8',dtype=str)

df = pd.pandas.read_csv(path3 +'titanic.csv',sep=",")

#dimension du data frame
print(df.shape)

#vérification de la version de scikit-learn
import sklearn
print(sklearn.__version__)

#information sur les variables
print(df.info())

#vérifier la distribution absolue des classes
print(df.value_counts())

#la distribution relative (je ne sais pas ce qu'il fait)
print(df.value_counts(normalize=True))

#donne la probabilité de mort ou vie
print(df.survived.value_counts(normalize=True))

#subdiviser les données en échantillons d'apprentissage et de test
from sklearn.model_selection import train_test_split
dfTrain, dfTest = train_test_split(df,test_size=300,random_state=1,stratify=df.survived)


#vérification des dimensions
print(dfTrain.shape) #(1009, 7)
print(dfTest.shape) #(300, 7)

#vérification des distributions en apprentissage
print(dfTrain.survived.value_counts(normalize=True))
print(dfTest.survived.value_counts(normalize=True))


#instanciation de l'arbre
from sklearn.tree import DecisionTreeClassifier
arbreFirst = DecisionTreeClassifier(min_samples_split=30,min_samples_leaf=10)



#reprendre ici
#construction de l'arbre
arbreFirst.fit(X = dfTrain.iloc[:,:-1], y = dfTrain.survived)


#arbreFirst.fit(X = df, y = df.survived)

DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
 max_depth=None, max_features=None, max_leaf_nodes=None,
 min_impurity_decrease=0.0, min_impurity_split=None,
min_samples_leaf=10, min_samples_split=30,
min_weight_fraction_leaf=0.0, presort='deprecated',
random_state=None, splitter='best')

##################################################################


##################################################################
#refait avec des valeurs numériques sur titanic2.csv


import pandas as pd
path3="C:/Users/Utilisateur/Documents/Python Scripts/Titanic/data/"
df2 = pd.pandas.read_csv(path3 +'titanic2.csv',sep=";")


#subdiviser les données en échantillons d'apprentissage et de test
from sklearn.model_selection import train_test_split
dfTrain2, dfTest2 = train_test_split(df2,test_size=300,random_state=1,stratify=df2.survived2)


#vérification des dimensions
print(dfTrain2.shape) #(1009, 7)
print(dfTest2.shape) #(300, 7)

#vérification des distributions en apprentissage
print(dfTrain2.survived2.value_counts(normalize=True))
print(dfTest2.survived2.value_counts(normalize=True))


#instanciation de l'arbre
from sklearn.tree import DecisionTreeClassifier
arbreFirst2 = DecisionTreeClassifier(min_samples_split=30,min_samples_leaf=10)


#a refaire pour la gestion des Nan
#construction de l'arbre [:,:,-1] retire la dernière colonne
#arbreFirst2.fit(X = dfTrain2.iloc[:,:-1], y = dfTrain2.survived2)
#ValueError: Input contains NaN, infinity or a value too large for dtype('float32').




#gestion des données manquantes
#Pour determiner si une colonne de la DataFRame contient une donnée manquante (NAN ou NULL) 
#on peut utiliser isnull et any, exemple:

df2.isnull().any()    
#Age contient des NAN


#Nombre de données manquantes par colonnes
df2.isnull().sum()

#Nombre de données manquantes pour la colonne age
df2['age'].isnull().sum()

##########################################################################
#Exemple de gestion de données manquantes par la fonction fillna
import pandas as pd
import numpy as np

df = pd.DataFrame({'X': [1, 2, 3, np.nan, 3],
                   'Y': [4, np.nan, 8, np.nan, 3]})
print("DataFrame:")
print(df)

filled_df = df.fillna(5)

print("Filled DataFrame:")
print(filled_df)

#remplace les nan par des 5

########################################################################

#je commence par remplacer les Nan par la moyenne des autres
#mieux serait de faire un modèle linéaire de l'age par rapport à une autre variable corrélée

df2.age.mean()
#29.881134512434034
#me revoit la moyenne sans les Nan



filled_df2=df2

filled_df2.age = df2.age.fillna(df2.age.mean())

print(filled_df2)



#vérification qu'il n'y a plus de donnée manquante
filled_df2.isnull().any() 
#False (c'est bon)

#j'en suis ici


#subdiviser les données en échantillons d'apprentissage et de test
from sklearn.model_selection import train_test_split
dfTrain2, dfTest2 = train_test_split(filled_df2,test_size=300,random_state=1,stratify=filled_df2.survived2)


#ici
#instanciation de l'arbre
from sklearn.tree import DecisionTreeClassifier
arbreFirst2 = DecisionTreeClassifier(min_samples_split=100,min_samples_leaf=50)



#arbreFirst2.fit(X = dfTrain2.iloc[:,:-1], y = dfTrain2.survived2)

#pour retrouver les résultats de R
#on va faire un arbre sans data test et train mais sur les 1309 obs

arbreFirst2.fit(X = filled_df2.iloc[:,:-1], y = filled_df2.survived2)


#arbreFirst.fit(X = df, y = df.survived)

DecisionTreeClassifier(ccp_alpha=0.00, class_weight=None, criterion='gini',
 max_depth=None, max_features=None, max_leaf_nodes=None,
 min_impurity_decrease=0.0, min_impurity_split=None,
min_samples_leaf=100, min_samples_split=5,
min_weight_fraction_leaf=0.0, presort='deprecated',
random_state=None, splitter='best')


#affichage graphique de l'arbre - depuis sklearn 0.21
#https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree
from sklearn.tree import plot_tree
plot_tree(arbreFirst2,feature_names = list(filled_df2.columns[:-1]),filled=True)

#on a ici un arbre qui s'affiche (mais illisible)

#affichage plus grand pour une meilleure lisibilité
import matplotlib.pyplot as plt
plt.figure(figsize=(50,50))
plot_tree(arbreFirst2,feature_names = list(filled_df2.columns[:-1]),filled=True)
plt.show()






########################################################################


#################################################################
#lien nouveau pour comprendre
#ici on utilise un datasets et non un dataframe
#https://kongakura.fr/article/arbre-de-decision-python
#test iris:
#Importation des modules nécessaires
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt
#importation du dataset Iris
iris= load_iris()
#Déclaration de l'arbre de décision
clf = DecisionTreeClassifier(max_depth=3)
#Entrainement de l'arbre de décision 
clf.fit(iris.data, iris.target)
#Affichage de l'abre de décision obtenu après entraînement
plot_tree(clf, feature_names= ['sepal_length','sepal_width','petal_length','petal_width'], class_names=["Iris-setosa","Iris-versicolor","Iris-virginica"],filled=True)
plt.show()


#afficher la table
import seaborn as sns
iris2 = sns.load_dataset('iris')
print(iris2.head())
################################################################


#################################################################

#lien openclassroom sur les données du titanic
#https://openclassrooms.com/fr/courses/4452741-decouvrez-les-librairies-python-pour-la-data-science/5574866-manipulez-les-donnees-contenues-dans-vos-dataframes

import numpy as np
import pandas as pd
import seaborn as sns
titanic = sns.load_dataset('titanic')

#outil en ligne pour exécuter du python
#https://hub.gke2.mybinder.org/user/ipython-ipython-in-depth-uxa81qo8/notebooks/binder/Untitled.ipynb?kernel_name=python3

###################################################################
